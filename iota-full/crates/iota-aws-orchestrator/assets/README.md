# iota-aws-orchestrator Benchmark Plotter

## Usage

**Important**: Always use `./run_plot.sh` instead of calling `python plot.py` directly. This ensures the correct Python environment with all dependencies is activated.

```bash
./run_plot.sh [options]
```

## Plot Types

1. **L-Graph (Latency-Throughput)**
   Shows the relationship between throughput (x-axis) and latency (y-axis).
2. **Health Plot (Load vs Throughput)**
   Displays input load vs actual throughput to detect saturation points.
3. **Scalability Plot**
   Shows how throughput scales with committee size under a latency constraint.
4. **Inspect Plots**
   Generate detailed per-client, per-workload throughput and latency over time.

## Command-Line Options

| Option                   | Default | Description                                                          |
| ------------------------ | ------- | -------------------------------------------------------------------- |
| `--dir`                  | `./`    | Data directory containing measurement JSON files                     |
| `--shared-objects-ratio` | `0 100` | Percentage of shared vs owned objects (space-separated list)         |
| `--committee`            | `4`     | Committee sizes to plot (space-separated list)                       |
| `--faults`               | `0`     | Number of faulty nodes to simulate (space-separated list)            |
| `--max-latencies`        | `2`     | Latency cap in seconds for scalability graphs (space-separated list) |
| `--y-max`                | None    | Maximum value for y-axis in L-graphs                                 |
| `--legend-columns`       | `1`     | Number of columns in the legend                                      |
| `--inspect`              | None    | Path to a single measurement file for detailed inspection            |
| `--precision`            | `30.0`  | Time granularity (seconds) for duration-based aggregation            |

## Input Data Format

The plotter expects JSON measurement files generated by the iota-aws-orchestrator benchmark runs. These files contain performance metrics collected from load generator clients.

### File Naming Convention

Measurement files follow this pattern:

```
measurements-<shared_ratio>-<faults>-<nodes>-<load>-<use_internal_ip>.json
```

**Example**: `measurements-30-0-4-1000-true.json`

- `30` - 30% shared objects, 70% owned objects
- `0` - 0 faulty nodes in the network
- `4` - 4 committee members (validator nodes)
- `1000` - 1000 tx/s target load
- `true` - Using internal IP addresses for communication

The script uses command-line arguments to construct filename patterns for loading measurement files. When arguments are not specified, wildcard patterns are used to match multiple files. This allows plotting graphs that compare different loads or configurations in a single visualization.

## Examples

### Inspect a specific measurement file

Plots the TPS, overall end-to-end latency, and end-to-end latency of different workload over time for the specified measurement file.

```bash
./run_plot.sh --inspect results/measurements-30-0-4-1000-true.json
```

### Basic benchmark visualization

Read and draw plots from file `measurements-30-0-4-*.json` and `measurements-50-0-4-*.json` in the `results/` directory. (The default value of `faults` is `0`)

```bash
./run_plot.sh --dir results/ --shared-objects-ratio 30 50 --committee 4
```

### Multiple committee sizes with latency constraints

Read and draw plots from file `measurements-30-0-4-*.json`, `measurements-30-0-7-*.json` and `measurements-30-0-10-*.json` in the `results/` directory. (The default value of `faults` is `0`)

Set the maximum latency to 2s and 5s for scalability plots, and set the maximum latency to 10s for L-graphs.

```bash
./run_plot.sh --dir results/ \
  --shared-objects-ratio 30 \
  --committee 4 7 10 \
  --max-latencies 2 5 \
  --y-max 10
```
